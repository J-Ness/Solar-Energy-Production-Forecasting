{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team Name**\n",
    "\n",
    "[146] hey girl lets ML n chill\n",
    "\n",
    "**Member, StudNr**\n",
    "\n",
    "Sinan Maric, 529195\n",
    "\n",
    "Joakim Ness, 544839\n",
    "\n",
    "Ole Lasson, 543928"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "raw_b = train_b\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_X(df):\n",
    "    data = df.copy()\n",
    "    if 'date_calc' in data.columns:\n",
    "        data.drop('date_calc', axis=1, inplace=True)\n",
    "\n",
    "    if 'date_forecast' in data.columns:\n",
    "        data.set_index('date_forecast', inplace=True)\n",
    "\n",
    "    \n",
    "    data = data.groupby(pd.Grouper(freq='1H')).mean()\n",
    "    \n",
    "    \n",
    "    data.dropna(how='all', inplace=True)\n",
    "    \n",
    "    data['hour'] = data.index.hour\n",
    "    data['week'] = pd.to_numeric(data.index.isocalendar().week)\n",
    "    data['week_hour'] = data['week'] * 100 + data['hour']\n",
    "    data['month'] = data.index.month\n",
    "\n",
    "\n",
    "    data['hour_sinus'] = np.sin(2*np.pi*data['hour']/ 23)\n",
    "    data['hour_cosine'] = np.cos(2*np.pi*data['hour'] / 23)\n",
    "\n",
    "    data['month_sinus'] = np.sin(2*np.pi*data['month'] / 12)\n",
    "    data['month_cosine'] = np.cos(2*np.pi*data['month'] / 12)\n",
    "\n",
    "    \n",
    "\n",
    "    data.drop(columns=['snow_density:kgm3', 'month', 'hour', 'week','week_hour'], inplace=True)\n",
    "    \n",
    "    data.rename_axis('time', inplace=True)\n",
    "\n",
    "    mapping = elevation_mapping = {\n",
    "            6.0 : 'A',\n",
    "            7.0 : 'B',\n",
    "            24.0 : 'C'\n",
    "        }\n",
    "    \n",
    "    data['location'] = data['elevation:m'].map(mapping)\n",
    "    data['location'] = data['location'].astype(str)\n",
    "    data.drop(columns=['elevation:m'], inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def remove_constants_Y(df):\n",
    "    data = df.copy()\n",
    "\n",
    "    # Create a mask to identify sequences of 3 or more consecutive rows with the same 'pv_measurement' (non-zero)\n",
    "    consecutive_mask = (data['pv_measurement'] == data['pv_measurement'].shift(1)) & (data['pv_measurement'] != 0)\n",
    "    consecutive_count = consecutive_mask.groupby((~consecutive_mask).cumsum()).cumsum()\n",
    "    mask_consecutive = (consecutive_count <= 3) | (~consecutive_mask)\n",
    "\n",
    "    #create a mask to identify sequences of 24 or more consecutive 0-valued pv-measurement\n",
    "    consecutive_mask_0 = (data['pv_measurement'] == data['pv_measurement'].shift(1)) & (data['pv_measurement'] == 0)\n",
    "    consecutive_count_0 = consecutive_mask_0.groupby((~consecutive_mask_0).cumsum()).cumsum()\n",
    "    mask_consecutive_0 = (consecutive_count_0 <= 24) | (~consecutive_mask_0)\n",
    "\n",
    "    combined_mask = mask_consecutive & mask_consecutive_0\n",
    "\n",
    "    filtered_data = data[combined_mask]\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def preprocessing_Y(df):\n",
    "    data = df.copy()\n",
    "    if 'time' in data.columns:\n",
    "        data.set_index('time', inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def add_estimated_flag(df, estimated):\n",
    "    data = df.copy()\n",
    "\n",
    "    data['estimated'] = 'E' if estimated else 'O'\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining observed and estimated data\n",
    "observed_a = preprocessing_X(X_train_observed_a)\n",
    "observed_a = add_estimated_flag(observed_a, False)\n",
    "\n",
    "estimated_a = preprocessing_X(X_train_estimated_a)\n",
    "estimated_a = add_estimated_flag(estimated_a, True)\n",
    "\n",
    "x_train_a = pd.concat([observed_a, estimated_a], axis=0)\n",
    "y_train_a = preprocessing_Y(train_a)\n",
    "m_train_a = x_train_a.merge(y_train_a, how='inner', on='time')\n",
    "\n",
    "observed_b = preprocessing_X(X_train_observed_b)\n",
    "observed_b = add_estimated_flag(observed_b, False)\n",
    "\n",
    "estimated_b = preprocessing_X(X_train_estimated_b)\n",
    "estimated_b = add_estimated_flag(estimated_b, True)\n",
    "\n",
    "x_train_b = pd.concat([observed_b, estimated_b], axis=0)\n",
    "y_train_b = preprocessing_Y(train_b)\n",
    "y_train_b = remove_constants_Y(y_train_b)\n",
    "m_train_b = x_train_b.merge(y_train_b, how='inner', on='time')\n",
    "\n",
    "observed_c = preprocessing_X(X_train_observed_c)\n",
    "observed_c = add_estimated_flag(observed_c, False)\n",
    "\n",
    "estimated_c = preprocessing_X(X_train_estimated_c)\n",
    "estimated_c = add_estimated_flag(estimated_c, True)\n",
    "\n",
    "x_train_c = pd.concat([observed_c, estimated_c], axis=0)\n",
    "y_train_c = preprocessing_Y(train_c)\n",
    "y_train_c = remove_constants_Y(y_train_c)\n",
    "m_train_c = x_train_c.merge(y_train_c, how='inner', on='time')\n",
    "\n",
    "x_train = pd.concat([m_train_a, m_train_b, m_train_c], axis=0, ignore_index=True)\n",
    "\n",
    "y_train = x_train['pv_measurement']\n",
    "\n",
    "x_train = x_train.drop(columns=['pv_measurement'], axis=1)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=10)\n",
    "\n",
    "X_train_a = X_train[X_train['location'] == 'A']\n",
    "X_train_b = X_train[X_train['location'] == 'B']\n",
    "X_train_c = X_train[X_train['location'] == 'C']\n",
    "\n",
    "Y_train_a = Y_train[X_train['location'] == 'A']\n",
    "Y_train_b = Y_train[X_train['location'] == 'B']\n",
    "Y_train_c = Y_train[X_train['location'] == 'C']\n",
    "\n",
    "X_val_a = X_val[X_val['location'] == 'A']\n",
    "X_val_b = X_val[X_val['location'] == 'B']\n",
    "X_val_c = X_val[X_val['location'] == 'C']\n",
    "\n",
    "Y_val_a = Y_val[X_val['location'] == 'A']\n",
    "Y_val_b = Y_val[X_val['location'] == 'B']\n",
    "Y_val_c = Y_val[X_val['location'] == 'C']\n",
    "\n",
    "x_test_a = preprocessing_X(X_test_estimated_a)\n",
    "x_test_a = add_estimated_flag(x_test_a, True)\n",
    "\n",
    "x_test_b = preprocessing_X(X_test_estimated_b)\n",
    "x_test_b = add_estimated_flag(x_test_b, True)\n",
    "\n",
    "x_test_c = preprocessing_X(X_test_estimated_c)\n",
    "x_test_c = add_estimated_flag(x_test_c, True)\n",
    "\n",
    "\n",
    "x_test = pd.concat([x_test_a, x_test_b, x_test_c], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "\n",
    "params = {\n",
    "    'depth' : 9,\n",
    "    'iterations' : 1000,\n",
    "    'loss_function' : 'MAE'\n",
    "}\n",
    "\n",
    "def catboost(X_train, Y_train, X_val, Y_val):\n",
    "    \n",
    "    model = CatBoostRegressor(**params)\n",
    "\n",
    "    train_pool = Pool(data=X_train, label=Y_train, cat_features=['location', 'estimated'])\n",
    "\n",
    "    eval_pool = Pool(data=X_val, label=Y_val, cat_features=['location', 'estimated'])\n",
    "\n",
    "    model.fit(train_pool, use_best_model=True, eval_set=eval_pool)\n",
    "    return model \n",
    "\n",
    "\n",
    "\n",
    "def train_predict(single=False):\n",
    "    lenlist = []\n",
    "    if not single:\n",
    "\n",
    "        model = catboost(X_train, Y_train, X_val, Y_val)\n",
    "        \n",
    "        predictions = pd.DataFrame(model.predict(x_test), columns=['prediction'])\n",
    "\n",
    "        predictions.index.name = 'id'\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    else:\n",
    "        model_a = catboost(X_train_a, Y_train_a, X_val_a, Y_val_a)\n",
    "        predict_a = model_a.predict(x_test_a)\n",
    "        predictions_a = pd.DataFrame(predict_a, columns=['prediction'])\n",
    "\n",
    "        model_b = catboost(X_train_b, Y_train_b, X_val_b, Y_val_b)\n",
    "        predict_b = model_b.predict(x_test_b)\n",
    "        predictions_b = pd.DataFrame(predict_b, columns=['prediction'])\n",
    "        \n",
    "        model_c = catboost(X_train_c, Y_train_c, X_val_c, Y_val_c)\n",
    "        predict_c = model_c.predict(x_test_c)\n",
    "        predictions_c = pd.DataFrame(predict_c, columns=['prediction'])\n",
    "\n",
    "        predictions = pd.concat([predictions_a, predictions_b, predictions_c], axis=0, ignore_index=True)\n",
    "\n",
    "        predictions.index.name = 'id'\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating 100 different train_test splits\n",
    "for i in range(100):\n",
    "    \n",
    "    #creating datasets based on seed state i\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=i)\n",
    "\n",
    "    X_train_a = X_train[X_train['location'] == 'A']\n",
    "    X_train_b = X_train[X_train['location'] == 'B']\n",
    "    X_train_c = X_train[X_train['location'] == 'C']\n",
    "\n",
    "    Y_train_a = Y_train[X_train['location'] == 'A']\n",
    "    Y_train_b = Y_train[X_train['location'] == 'B']\n",
    "    Y_train_c = Y_train[X_train['location'] == 'C']\n",
    "\n",
    "    X_val_a = X_val[X_val['location'] == 'A']\n",
    "    X_val_b = X_val[X_val['location'] == 'B']\n",
    "    X_val_c = X_val[X_val['location'] == 'C']\n",
    "\n",
    "    Y_val_a = Y_val[X_val['location'] == 'A']\n",
    "    Y_val_b = Y_val[X_val['location'] == 'B']\n",
    "    Y_val_c = Y_val[X_val['location'] == 'C']\n",
    "\n",
    "\n",
    "    temp_catboost_prediction = train_predict(single=True)\n",
    "\n",
    "\n",
    "    #creating master dataframe\n",
    "    if i == 0:\n",
    "        catboost_master = pd.DataFrame()\n",
    "\n",
    "    #adding prediction to master dataframe\n",
    "    catboost_master[f'prediction_{i}'] = temp_catboost_prediction['prediction']\n",
    "        \n",
    "\n",
    "#establishing final dataframe\n",
    "catboost_predictions = pd.DataFrame()\n",
    "\n",
    "#calculating average for printout\n",
    "catboost_master['avg'] = catboost_master.mean(axis=1)\n",
    "\n",
    "#calculating mean\n",
    "catboost_predictions['prediction'] = catboost_master.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(features_df, preds_df):\n",
    "    features = features_df.copy()\n",
    "    preds = preds_df.copy()\n",
    "\n",
    "    # Set the index of 'features' to match 'preds'\n",
    "    features['is_day:idx'].index = preds.index\n",
    "\n",
    "    # Setting all night-time predictions to zero\n",
    "    preds.loc[features['is_day:idx'] == 0, 'prediction'] = 0\n",
    "\n",
    "    # Setting all low values to zero\n",
    "    preds['prediction'] = preds['prediction'].apply(lambda x: 0.0 if x < 1 else x)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_predictions_proc = postprocessing(x_test, catboost_predictions)\n",
    "\n",
    "catboost_predictions_proc.index.name = 'id'\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "catboost_predictions_proc.to_csv(f'pred_{current_time}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
